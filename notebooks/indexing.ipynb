{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2054a0b3",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99c7d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_text_splitters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_text_splitters'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "FILTERED_DATA_PATH = Path('../data/filtered_complaints.csv')\n",
    "VECTOR_STORE_PATH = Path('../vector_store')\n",
    "FAISS_INDEX_PATH = VECTOR_STORE_PATH / 'faiss_index.bin'\n",
    "METADATA_PATH = VECTOR_STORE_PATH / 'metadata.pkl'\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# Embedding model\n",
    "EMBEDDING_MODEL = 'sentence-transformers/paraphrase-MiniLM-L3-v2'\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "# For demo, limit rows (set to None for full dataset)\n",
    "DEMO_LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILTERED_DATA_PATH)\n",
    "print(f\"Total complaints: {len(df):,}\")\n",
    "\n",
    "if DEMO_LIMIT:\n",
    "    df = df.head(DEMO_LIMIT)\n",
    "    print(f\"Using demo subset: {len(df):,} complaints\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Example: chunk a single narrative\n",
    "sample_narrative = df['narrative'].iloc[0]\n",
    "sample_chunks = splitter.split_text(sample_narrative)\n",
    "\n",
    "print(f\"Original length: {len(sample_narrative)} chars\")\n",
    "print(f\"Number of chunks: {len(sample_chunks)}\")\n",
    "print(f\"\\nFirst chunk ({len(sample_chunks[0])} chars):\")\n",
    "print(sample_chunks[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all complaints\n",
    "chunks = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking\"):\n",
    "    narrative = row['narrative']\n",
    "    if pd.isna(narrative) or not narrative.strip():\n",
    "        continue\n",
    "    \n",
    "    text_chunks = splitter.split_text(narrative)\n",
    "    \n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        chunk_id = hashlib.md5(f\"{row['complaint_id']}_{i}\".encode()).hexdigest()\n",
    "        \n",
    "        chunks.append({\n",
    "            'id': chunk_id,\n",
    "            'text': chunk_text,\n",
    "            'metadata': {\n",
    "                'complaint_id': str(row['complaint_id']),\n",
    "                'product': row['product'],\n",
    "                'product_original': row['product_original'] if pd.notna(row['product_original']) else '',\n",
    "                'issue': row['issue'] if pd.notna(row['issue']) else '',\n",
    "                'company': row['company'] if pd.notna(row['company']) else '',\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(text_chunks)\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(chunks):,} chunks from {len(df):,} complaints\")\n",
    "print(f\"Average chunks per complaint: {len(chunks)/len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "Model: sentence-transformers/paraphrase-MiniLM-L3-v2\n",
    "Embedding dimension: 384\n",
    "# Generate embeddings\n",
    "texts = [c['text'] for c in chunks]\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed195ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index (L2 distance)\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "\n",
    "# Add embeddings\n",
    "embeddings_float32 = embeddings.astype('float32')\n",
    "index.add(embeddings_float32)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal:,} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93790232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata for storage\n",
    "metadata_list = []\n",
    "for c in chunks:\n",
    "    metadata_list.append({\n",
    "        'id': c['id'],\n",
    "        'text': c['text'],\n",
    "        **c['metadata']\n",
    "    })\n",
    "\n",
    "print(f\"Metadata entries: {len(metadata_list):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb03cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "test_query = \"billing dispute credit card\"\n",
    "query_embedding = model.encode([test_query], convert_to_numpy=True).astype('float32')\n",
    "\n",
    "k = 5\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for i, (dist, idx) in enumerate(zip(distances[0], indices[0]), 1):\n",
    "    meta = metadata_list[idx]\n",
    "    print(f\"\\n{i}. [Distance: {dist:.4f}] Product: {meta['product']}\")\n",
    "    print(f\"   Issue: {meta['issue']}\")\n",
    "    print(f\"   Text: {meta['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05199c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save demo index\n",
    "# VECTOR_STORE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "# faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
    "# with open(METADATA_PATH, 'wb') as f:\n",
    "#     pickle.dump(metadata_list, f)\n",
    "# print(\"Index saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
