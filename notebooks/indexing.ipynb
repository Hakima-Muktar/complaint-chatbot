{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2054a0b3",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99c7d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5b0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "FILTERED_DATA_PATH = Path('../data/filtered_complaints.csv')\n",
    "VECTOR_STORE_PATH = Path('../vector_store')\n",
    "FAISS_INDEX_PATH = VECTOR_STORE_PATH / 'faiss_index.bin'\n",
    "METADATA_PATH = VECTOR_STORE_PATH / 'metadata.pkl'\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# Embedding model\n",
    "EMBEDDING_MODEL = 'sentence-transformers/paraphrase-MiniLM-L3-v2'\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "# For demo, limit rows (set to None for full dataset)\n",
    "DEMO_LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c89b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2660\\3087027629.py:1: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Project\\complaint-chatbot\\data\\processed\\complaints.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total complaints: 9,609,797\n",
      "Using demo subset: 1,000 complaints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Information belongs to someone else</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experian Information Solutions Inc.</td>\n",
       "      <td>FL</td>\n",
       "      <td>32092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14195687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Telecommunications debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Company can't verify or dispute the facts in t...</td>\n",
       "      <td>Eastern Account Systems of Connecticut, Inc.</td>\n",
       "      <td>FL</td>\n",
       "      <td>342XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14195688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Reporting company used your report improperly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14195689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Reporting company used your report improperly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experian Information Solutions Inc.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14195690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Account status incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experian Information Solutions Inc.</td>\n",
       "      <td>IL</td>\n",
       "      <td>60628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2025-06-20</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14195692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                                            Product  \\\n",
       "0    2025-06-20  Credit reporting or other personal consumer re...   \n",
       "1    2025-06-20                                    Debt collection   \n",
       "2    2025-06-20  Credit reporting or other personal consumer re...   \n",
       "3    2025-06-20  Credit reporting or other personal consumer re...   \n",
       "4    2025-06-20  Credit reporting or other personal consumer re...   \n",
       "\n",
       "               Sub-product                                 Issue  \\\n",
       "0         Credit reporting  Incorrect information on your report   \n",
       "1  Telecommunications debt     Attempts to collect debt not owed   \n",
       "2         Credit reporting           Improper use of your report   \n",
       "3         Credit reporting           Improper use of your report   \n",
       "4         Credit reporting  Incorrect information on your report   \n",
       "\n",
       "                                       Sub-issue Consumer complaint narrative  \\\n",
       "0            Information belongs to someone else                          NaN   \n",
       "1                              Debt is not yours                          NaN   \n",
       "2  Reporting company used your report improperly                          NaN   \n",
       "3  Reporting company used your report improperly                          NaN   \n",
       "4                       Account status incorrect                          NaN   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                                NaN   \n",
       "1  Company can't verify or dispute the facts in t...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                        Company State ZIP code Tags  \\\n",
       "0           Experian Information Solutions Inc.    FL    32092  NaN   \n",
       "1  Eastern Account Systems of Connecticut, Inc.    FL    342XX  NaN   \n",
       "2        TRANSUNION INTERMEDIATE HOLDINGS, INC.    AZ    85225  NaN   \n",
       "3           Experian Information Solutions Inc.    AZ    85225  NaN   \n",
       "4           Experian Information Solutions Inc.    IL    60628  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0                        NaN           Web           2025-06-20   \n",
       "1                        NaN           Web           2025-06-20   \n",
       "2                        NaN           Web           2025-06-20   \n",
       "3                        NaN           Web           2025-06-20   \n",
       "4                        NaN           Web           2025-06-20   \n",
       "\n",
       "  Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0                  In progress              Yes                NaN   \n",
       "1      Closed with explanation              Yes                NaN   \n",
       "2                  In progress              Yes                NaN   \n",
       "3                  In progress              Yes                NaN   \n",
       "4                  In progress              Yes                NaN   \n",
       "\n",
       "   Complaint ID  \n",
       "0      14195687  \n",
       "1      14195688  \n",
       "2      14195689  \n",
       "3      14195690  \n",
       "4      14195692  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Project\\complaint-chatbot\\data\\processed\\complaints.csv\")\n",
    "print(f\"Total complaints: {len(df):,}\")\n",
    "\n",
    "if DEMO_LIMIT:\n",
    "    df = df.head(DEMO_LIMIT)\n",
    "    print(f\"Using demo subset: {len(df):,} complaints\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b24a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 51 chars\n",
      "Number of chunks: 1\n",
      "\n",
      "First chunk (51 chars):\n",
      "Credit reporting or other personal consumer reports...\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Example: chunk a single narrative\n",
    "sample_narrative = df['Product'].iloc[0]\n",
    "sample_chunks = splitter.split_text(sample_narrative)\n",
    "\n",
    "print(f\"Original length: {len(sample_narrative)} chars\")\n",
    "print(f\"Number of chunks: {len(sample_chunks)}\")\n",
    "print(f\"\\nFirst chunk ({len(sample_chunks[0])} chars):\")\n",
    "print(sample_chunks[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dadad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chunking: 100%|██████████| 1000/1000 [00:00<00:00, 1470.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 1,000 chunks from 1,000 complaints\n",
      "Average chunks per complaint: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Chunk all complaints\n",
    "chunks = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking\"):\n",
    "    narrative = row['Product']\n",
    "    if pd.isna(narrative) or not narrative.strip():\n",
    "        continue\n",
    "    \n",
    "    text_chunks = splitter.split_text(narrative)\n",
    "    \n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        chunk_id = hashlib.md5(f\"{row['Complaint ID']}_{i}\".encode()).hexdigest()\n",
    "        \n",
    "        chunks.append({\n",
    "            'id': chunk_id,\n",
    "            'text': chunk_text,\n",
    "            'metadata': {\n",
    "                'Complaint ID': str(row['Complaint ID']),\n",
    "                'product': row['Product'],\n",
    "                'Sub-product': row['Sub-product'] if pd.notna(row['Sub-product']) else '',\n",
    "                'Issue': row['Issue'] if pd.notna(row['Issue']) else '',\n",
    "                'Company': row['Company'] if pd.notna(row['Company']) else '',\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(text_chunks)\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(chunks):,} chunks from {len(df):,} complaints\")\n",
    "print(f\"Average chunks per complaint: {len(chunks)/len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d49b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L3-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sentence-transformers/paraphrase-MiniLM-L3-v2\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d9ad5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1000, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ed195ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 1,000 vectors\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index (L2 distance)\n",
    "index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "\n",
    "# Add embeddings\n",
    "embeddings_float32 = embeddings.astype('float32')\n",
    "index.add(embeddings_float32)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal:,} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93790232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata entries: 1,000\n"
     ]
    }
   ],
   "source": [
    "# Prepare metadata for storage\n",
    "metadata_list = []\n",
    "for c in chunks:\n",
    "    metadata_list.append({\n",
    "        'id': c['id'],\n",
    "        'text': c['text'],\n",
    "        **c['metadata']\n",
    "    })\n",
    "\n",
    "print(f\"Metadata entries: {len(metadata_list):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceb03cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'billing dispute credit card'\n",
      "\n",
      "Top 5 results:\n",
      "\n",
      "1. [Distance: 22.9198] product: Credit card\n",
      "   Issue: Problem with a company's investigation into an existing problem\n",
      "   Text: Credit card...\n",
      "\n",
      "2. [Distance: 22.9198] product: Credit card\n",
      "   Issue: Problem with a company's investigation into an existing problem\n",
      "   Text: Credit card...\n",
      "\n",
      "3. [Distance: 22.9198] product: Credit card\n",
      "   Issue: Problem with a company's investigation into an existing problem\n",
      "   Text: Credit card...\n",
      "\n",
      "4. [Distance: 22.9198] product: Credit card\n",
      "   Issue: Problem with a company's investigation into an existing problem\n",
      "   Text: Credit card...\n",
      "\n",
      "5. [Distance: 22.9198] product: Credit card\n",
      "   Issue: Incorrect information on your report\n",
      "   Text: Credit card...\n"
     ]
    }
   ],
   "source": [
    "# Test semantic search\n",
    "test_query = \"billing dispute credit card\"\n",
    "query_embedding = model.encode([test_query], convert_to_numpy=True).astype('float32')\n",
    "\n",
    "k = 5\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for i, (dist, idx) in enumerate(zip(distances[0], indices[0]), 1):\n",
    "    meta = metadata_list[idx]\n",
    "    print(f\"\\n{i}. [Distance: {dist:.4f}] product: {meta['product']}\")\n",
    "    print(f\"   Issue: {meta['Issue']}\")\n",
    "    print(f\"   Text: {meta['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05199c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save demo index\n",
    "# VECTOR_STORE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "# faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
    "# with open(METADATA_PATH, 'wb') as f:\n",
    "#     pickle.dump(metadata_list, f)\n",
    "# print(\"Index saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
